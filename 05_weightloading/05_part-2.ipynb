{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0599d57c-16e4-478c-954d-89dbbd193ced",
   "metadata": {},
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b82151-07c6-4867-b374-741258033b52",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 5) Loading pretrained weights (part 2; using LitGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617b8f-8493-4afa-8c91-f3d1ab79795b",
   "metadata": {},
   "source": [
    "- Now, we are loading the weights using an open-source library called LitGPT\n",
    "- LitGPT is fundamentally similar to the LLM code we implemented previously, but it is much more sophisticated and supports more than 20 different LLMs (Mistral, Gemma, Llama, Phi, and more)\n",
    "\n",
    "# ⚡ LitGPT\n",
    "\n",
    "**20+ high-performance LLMs with recipes to pretrain, finetune, deploy at scale.**\n",
    "\n",
    "<pre>\n",
    "✅ From scratch implementations     ✅ No abstractions    ✅ Beginner friendly   \n",
    "✅ Flash attention                  ✅ FSDP               ✅ LoRA, QLoRA, Adapter\n",
    "✅ Reduce GPU memory (fp4/8/16/32)  ✅ 1-1000+ GPUs/TPUs  ✅ 20+ LLMs            \n",
    "</pre>\n",
    "\n",
    "## Basic usage:\n",
    "\n",
    "```\n",
    "# ligpt [action] [model]\n",
    "litgpt  download  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  chat      meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  evaluate  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  finetune  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  pretrain  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  serve     meta-llama/Meta-Llama-3-8B-Instruct\n",
    "```\n",
    "\n",
    "\n",
    "- You can learn more about LitGPT in the [corresponding GitHub repository](https://github.com/Lightning-AI/litgpt), that contains many tutorials, use cases, and examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install litgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cf71fa-af17-4c72-a6ab-f258a2b5a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "litgpt version: 0.5.3\n",
      "torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"litgpt\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29baa9-c3b0-493d-94b4-eaa8146d6b3c",
   "metadata": {},
   "source": [
    "- First, let's see what LLMs are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae8df66-f391-4266-b437-a1f601a6ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify --repo_id <repo_id>. Available values:\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "databricks/dolly-v2-7b\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "h2oai/h2o-danube2-1.8b-chat\n",
      "keeeeenw/MicroLlama\n",
      "lmsys/longchat-13b-16k\n",
      "lmsys/longchat-7b-16k\n",
      "lmsys/vicuna-13b-v1.3\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-33b-v1.3\n",
      "lmsys/vicuna-7b-v1.3\n",
      "lmsys/vicuna-7b-v1.5\n",
      "lmsys/vicuna-7b-v1.5-16k\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-128k-instruct\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "NousResearch/Nous-Hermes-13b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495037e-0068-49ad-9bed-0bcdc440727d",
   "metadata": {},
   "source": [
    "- We can then download an LLM via the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!litgpt download microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf5be0-4aa1-498f-b08a-68ff234cbea5",
   "metadata": {},
   "source": [
    "- And there's also a Python API to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e057edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wengz\\miniforge3\\envs\\ds_gpu\\Lib\\site-packages\\litgpt\\model.py:346: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Llamas are herbivorous animals, primarily grazing on low grasses and shrubs. They can consume a variety of plants including grass, clover, alfalfa, and tough brush. In the wild, llam'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(r\"..\\checkpoints\\microsoft\\Phi-3-mini-4k-instruct\")\n",
    "\n",
    "llm.generate(\"What do Llamas eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc775d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llamas, being herbivores, primarily feed on a variety of grasses, hay, and occasionally supplement their diet with grains. In their native Peruvian Andes, where they roam the high-altitude grasslands, llamas consume a mix of foliage, including diverse grass and shrub species adapted to the harsh environmental conditions. Their diet can be naturally supplemented with the Spirulina, a blue-green algae that is rich in protein, and they often consume small quantities of tree bark and lichen, which may help with the passage of tougher plant materials.\n",
      "\n",
      "\n",
      "In captivity or in regions where their natural diet may be supplemented, llamas are often fed commercial llamas hay, alfalfa, and a balanced mix of nutrients to ensure their dietary requirements are met, particularly during pregnancy, lactation, or when they"
     ]
    }
   ],
   "source": [
    "result = llm.generate(\"What do Llamas eat?\", stream=True, max_new_tokens=200)\n",
    "for e in result:\n",
    "    print(e, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288158da",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Exercise 2: Download an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717b67f",
   "metadata": {},
   "source": [
    "- Download and try out an LLM of your own choice (recommendation: 7B parameters or smaller)\n",
    "- We will finetune the LLM in the next notebook\n",
    "- You can also try out the `litgpt chat` command from the terminal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
